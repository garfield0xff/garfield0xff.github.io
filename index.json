[{"content":"다양한 문제에서 Loss를 어떻게 정의하고 \b줄여나갈 수 있을까? Gradient descent를 통해 한번 알아보자\n정의 주어진 볼록 함수를 로컬 최소값으로 최소화 하기위해 매개 변수를 반복적으로 조정하는 기법 매개변수 ( weights 에 대한 Loss )를 반복적으로 조정해 \b볼록 함수의 최소화하는 변곡점을 찾는 과정이라고 해석 할 수 있다.\n제약 조건이 없는 수학적 최적화 방법 경사 하강법은 일반적으로 단순히 함수의 기울기를 따라 이동하며 최솟값을 찾는 문제이기 떄문에 제약 조건이 없는 최적화 문제에 사용된다. 제약 조건이 있을 경우에는 추가적인 ( 라그랑주 승수법, 베리어 함수법)등을 기법을 수행해야한다\n이해 아래 두 그림을 보면서 이해해 보도록 하자\n위 그림은 Weights에 대한 Loss의 기울기를 그리며 learning rate 비율 만큼 움직이는 것을 확인 할 수 있다. 전에 설명했던 MSE Loss function을 사용한다고 가정 할 경우 원하는 선형이 Loss의 최저점을 찍고 올라가면서 위 그래프처럼 볼록함수를 그리며 올라갈 것이므로 이 비율(learning reate)를 조정한다면 최저점을 찾아 갈 수 있을 것이다.\n위 그림은 제약 조건이 없이 기울기를 따라가며 최저점을 찾는 과정을 보여주고 있다. 정의하고 싶은 Loss에 따라 추가적인 기법이 필요하겠지만 일반적으로 위와 같이 단순히 매개변수를 조정하며 최소화하는 변곡점을 찾는 과정이라고 생각할 수 있다.\n설계 파이프 라인 Gradient Descent Limitation gradient descent에는 크게 두가지 한계점이 존재한다\nLocal minimum : gradient descent는 함수 기울기를 따라 이동하기 때문에 Global minimum이 아닌 local minimum에 도달해 epoch로 확인해보지 않는 이상 local minimum에서 나오지 못할 수 있다. 특히 non-convex 일떄 문제가 될 수 있다.\nComputational Cost : 전통적인 Batch Gradient Descent는 전체 데이터셋을 사용하여 경사를 계산하여 대규모 데이터셋에서는 계산 cost이 매우 높아 사용하기 힘들다.\nStochastic Gradient Descent ( SGD ) 위의 전통적인 Gradient Descent의 한계를 극복하기 위해 나온것이 SGD이다. SGD에서는 무작위로 선택된 단일 데이터 포인터에 대해 손실 함수의 경사를 계산해 계산 cost 를 줄이고 local minimum에 대해 벗어날 수 있는 선택지를 제공 할 수 있다. 현재의 optimal function 은 이 원리를 따르는 경우가 많다\n자료 출처 : builtinfreesky\n","permalink":"https://garfield0xff.github.io/posts/gradient-descent/","summary":"다양한 문제에서 Loss를 어떻게 정의하고 \b줄여나갈 수 있을까? Gradient descent를 통해 한번 알아보자\n정의 주어진 볼록 함수를 로컬 최소값으로 최소화 하기위해 매개 변수를 반복적으로 조정하는 기법 매개변수 ( weights 에 대한 Loss )를 반복적으로 조정해 \b볼록 함수의 최소화하는 변곡점을 찾는 과정이라고 해석 할 수 있다.\n제약 조건이 없는 수학적 최적화 방법 경사 하강법은 일반적으로 단순히 함수의 기울기를 따라 이동하며 최솟값을 찾는 문제이기 떄문에 제약 조건이 없는 최적화 문제에 사용된다. 제약 조건이 있을 경우에는 추가적인 ( 라그랑주 승수법, 베리어 함수법)등을 기법을 수행해야한다","title":"Gradient Descent"},{"content":"컴퓨터 비전에 대해 원론적으로 접근해보면서 컴퓨터 비전에 대한 Depth를 넓히고자 한다 .\n정의 디지털 이미지에서 정보를 추출하는 과학적인 영역 여기서 말하는 정보란 (Space, Location.. ) 등의 새로운 차원의 정보를 말한다. 따라서 RGB 혹은 RGBA 데이터로 새로운 차원의 정보를 추출하는 과학적인 영역이라고 해석할 수 있다\n인간의 시야와 비슷한 시야 인지 능력을 구현하는 학문 인간의 지각 능력과 인지(해석)능력을 구분해 파이프라인을 구현하고 시야인지 능력을 구현하고자 하는 학문이라고 해석할 수 있다.\n아래 이미지를 통해 컴퓨터 비전이 포맷에 따라 어떻게 사용되는지 간략하게 파악 할 수 있다.\n이해 위 정의를 통해 컴퓨터 비전을 이해한다는 것은 인간의 시야 인지 능력을 컴퓨터로 구현하는 것을 이해해야한다 라고도 해석 할 수 있을 것이다. 그러면 순차적으로 어떤 것들을 이해해야 하는지 살펴보자.\ncomputer science : memory, data structure 등 이미지를 처리하는 자료구조 및 메모리에 대한 기본적인 이해가 필요하다. algorithm : Filtering, Edge Detection 등 이미지 전처리를 위한 전통적인 알고리즘 기법을 이해해야한다. machine learning: 다양한 조건에서 나오는 이미지 패턴에 대한 일반화, 복잡한 연산을 통한 특징점 검출, 복잡한 이미지 패턴을 추출하기 위해 maching learning 알고리즘에 대한 기본적인 이해가 필요하다. 그렇다면 machine learning은 컴퓨터 비전에서 어떻게 설계 되었을까? 인간의 sensing device와 interpreting device를 구분해서 해석하고 알고리즘으로 구현한 것이 machine learning의 기본 원리라고 이해 할 수 있다. 아래 인간의 시야 인지 파이프라인을 간략화한 그림을 통해 컴퓨터 비전이 어떻게 처리되는지 간단하게 이해해보자.\n파이프라인 빛이 인간의 눈에 들어오면 망막의 시세포가 빛을 신경 신호로 변환해준다. 여기서 변환된 신경 신호는 외측 슬상체를 통해 두 눈에서 들어온 신호를 결합하고 신호정보의 전처리와 필터링을 수행해준다.\n전처리된 정보는 1차 시각야에서 윤곽을 추출해주는 방위 선택성 뉴런에 반응하게 된다. 방위 선택성 뉴런은 반응하는 방식에 따라 단순형 세포와 복잡형 세포로 나뉜다. 단순형 세포는 특정 위치와 특정 방향에 정확하게 위치한 선에 반응하고 복잡형 세포는 시야 어느 위치에 있든 특정 방향으로 기울어진 선에 반응하게 된다. 따라서 단순형 세포는 밝은 영역과 어두운 영역을 명확히 구분하는데 도움을 주고 복잡형 세포는 움직이는 물체나 선의 연속적인 변화를 인식하는데 도움을 준다. 딥러닝 알고리즘 중 CNN에서 Convolution 연산이 단순형 세포의 역활을 하고 Polling 연산이 복잡한 세포의 역활을 맡는다고도 생각 할 수 있다.\n2차 시각야에서는 1차 시각야에서 추출된 윤곽 및 시각 특징들의 정보를 통합하고 제 4차 시각야로 보내준다. (2차 시각야에서도 1차 시각야의 방위선택성 뉴런이 존재하여 이미지 윤곽을 추출하는 역활을 하지만 주요 역활은 시각 정보 특징들의 정보를 통합하는 것이라고 이해 할 수 있다.)\n4차 시각야에서는 통합된 정보를 바탕으로 색상, 형태 ,물체를 인식하고 물체에 대한 이해 및 상위 피질과의 중계 역활을 한다. 따라서 인간의 interpreting device의 역활을 일부 수행한다고 볼 수 있다 .\n파이프라인 정리 정리해보면 다음과 같다.\n망막 : 빛을 신경 세포로 변환 외측 슬상체 : 신호 결합 및 전처리 1차 시각야 : 신호 정보에 대한 윤곽 및 시야정보 추출 2차 시각야: 시야 정보 통합 및 고차원 시야 정보 추출 4차 시각야: 고차원의 시야 정보를 바탕으로 색상, 형태, 물체 인식 및 분류 및 상위 피질과의 중계 우리는 이것을 컴퓨터 프로세스에 대입해서 생각해 볼 수 있다.\n카메라 센서 및 광학 센서 ( 망막 ) : 빛을 전자 신호로 변환 아날로그-디지털 변환 ( 외측 슬상체 ): 전자 신호를 이미지 데이터로 변환 및 전처리 윤곽 추출 알고리즘 ( 1차 시각야 ) : 이미지 데이터에 윤곽 추출 알고리즘을 거쳐 윤곽 생성 ex) CNN, ResNet\u0026hellip; 시야 정보 통합 알고리즘 ( 2차 시각야 ) : 추출한 정보 데이터를 통합하고 고차원의 시야정보 생성 ex) CNN, ResNet.. 색상, 형태, 물체 인식 알고리즘 ( 4차 시각야 ) : 고차원의 시야정보로 물체를 인식 혹은 판별 ex) R-CNN, YOLO, CNN, GAN.. 위의 인지 프로세스를 이해했다면 아래 CNN의 과정을 대략적으로 이해 할 수 있을 것이다.\n다음시간에는 딥러닝 및 머신러닝의 기본 원리, 학습의 의미를 표현하고 있는 Linear Regression에 대해 알아보고자 한다.\n참고 : standford computer vision lab,helloT\n이미지 출처 : Analytics Vidhya, standford computer vision lab, helloT\n","permalink":"https://garfield0xff.github.io/posts/introduction-cv/","summary":"컴퓨터 비전에 대해 원론적으로 접근해보면서 컴퓨터 비전에 대한 Depth를 넓히고자 한다 .\n정의 디지털 이미지에서 정보를 추출하는 과학적인 영역 여기서 말하는 정보란 (Space, Location.. ) 등의 새로운 차원의 정보를 말한다. 따라서 RGB 혹은 RGBA 데이터로 새로운 차원의 정보를 추출하는 과학적인 영역이라고 해석할 수 있다\n인간의 시야와 비슷한 시야 인지 능력을 구현하는 학문 인간의 지각 능력과 인지(해석)능력을 구분해 파이프라인을 구현하고 시야인지 능력을 구현하고자 하는 학문이라고 해석할 수 있다.\n아래 이미지를 통해 컴퓨터 비전이 포맷에 따라 어떻게 사용되는지 간략하게 파악 할 수 있다.","title":"Introduction Computer Vision"},{"content":"학습의 과정은 물체 혹은 이미지를 지각하고 인지한 후 내 머릿속의 식을 바꾸는 것으로 해석할 수 있다. 그렇다면 Linear Legression은 학습을 어떻게 표현했을지 알아보자.\n정의 데이터를 사용하여 변수 간의 관계를 식별하고 이러한 관계를 사용하여 예측을 수행하는 기술 x_data 와 y_data 가 주어졌을떄 두 데이터간의 관계를 파악하고 새로운 z_data 에 대한 예측을 수행하는 기술이라고 이해 할 수 있다.\n\b고차원 공간에서 데이터 점들을 가장 잘 표현할 수 있는 초평면(hyperplane)을 찾는 기술 여기서 초평면은 n 차원 공간에서 n -1 차원의 flat한 면이다. 따라서 고차원에서 점들을 n -1 차원의 평면으로 표현 할 수 있는 기술이라고 이해 할 수 있다.\n이해 일차함수로 Linear Regression을 이해해보도록하자.\n(x_data, y_data) 들의 관계의 이상적인 관계는 아래와 같이 녹색 선으로 생각 할 수 있을 것이다.\n그렇다면 어떻게 녹색선이 x_data 와 y_data 를 잘 표현했다고 수치적으로 나타낼 수 있을까? maching learning에서는 loss와 error 의 개념을 사용해 정의한다.\nerror : 예측값과 실제 값 사이의 차이를 의미한다. 위에서는 빨간 점과 녹색선의 거리를 의미하며 일반적으로 단일 데이터 포인트에 대해 계산한다.\nloss : 모델의 전체 성능을 평가하기 위한 지표로 사용되며 error들의 집합적 척도로 여러 데이터 포인터에 대해 계산된다. 위에서는 빨간점 과 녹색선의 거리들의 집합 척도로 생각할 수 있다. 모델에 따라 loss (집합의 척도)를 다르게 표현해야하며 원하는 선형을 표현하기 위해 다양한 loss들을 시도해 볼 수 있다.\n따라서 위 녹색선은 Error 집합들의 척도가 작다는 것을 머릿속으로 인지했기 떄문에 이상적인 관계로 결론을 낸 것이다.\n그렇다면 loss 는 어떻게 error의 척도를 나타낼 수 있을까? 아래 그림은 각 데이터에 대한 error 들을 나타낸 그림이다.\n처음엔 loss를 아래와 같이 정의할 수 있을 것이다.\n$$ e1 + e2 + e3 + e4 + e5 \u0026hellip; = loss $$ 하지만 위 식에는 문제점이 있다. 아래의 그림을 보자.\n위 그림과 아래 그림 모두 로스는 0을 가리키지만 아래 그림은 경사(slope)를 가지지 않는 y = b 의 기울기가 그려지게 된다. 따라서 error가 상쇄돼 모델의 성능을 왜곡시킴에도 불구하고 loss 값이 적기 때문에 성능이 좋은 것 처럼 보이게 할 수 있다.\n이를 방지하기 위해 error를 절댓값으로 변환하거나 제곱하여 loss 를 표현한다\n평균 절대 오차 ( Mean Absolute Error, MAE) \\[ \\frac{\\partial \\text{MSE}}{\\partial \\beta_j} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i) x_{ij} \\] 평균 제곱 오차 ( Mean Squared Error, MSE) $$ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |e_i| $$\n위 loss들을 경사 하강법을 사용하여 예측 함수의 기울기를 업데이트 할 수 있다. (경사하강법에 대해서는 다음시간에 자세하게 알아볼것이다.)\n$$ \\frac{\\partial \\text{MSE}}{\\partial \\beta_j} = -\\frac{2}{n} \\sum_{i=1}^{n} (e_i) x_{ij} $$\n마지막으로 매개변수를 업데이트 해주면 한번의 alpha 비율의 학습이 완료된 것이다.\n$$ \\beta_j := \\beta_j - \\alpha \\frac{\\partial \\text{MSE}}{\\partial \\beta_j} $$\n","permalink":"https://garfield0xff.github.io/posts/linear-regression/","summary":"학습의 과정은 물체 혹은 이미지를 지각하고 인지한 후 내 머릿속의 식을 바꾸는 것으로 해석할 수 있다. 그렇다면 Linear Legression은 학습을 어떻게 표현했을지 알아보자.\n정의 데이터를 사용하여 변수 간의 관계를 식별하고 이러한 관계를 사용하여 예측을 수행하는 기술 x_data 와 y_data 가 주어졌을떄 두 데이터간의 관계를 파악하고 새로운 z_data 에 대한 예측을 수행하는 기술이라고 이해 할 수 있다.\n\b고차원 공간에서 데이터 점들을 가장 잘 표현할 수 있는 초평면(hyperplane)을 찾는 기술 여기서 초평면은 n 차원 공간에서 n -1 차원의 flat한 면이다.","title":"Linear Regression"}]